{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48cf6310",
   "metadata": {},
   "source": [
    "에이전트의 히스토리를 관리하는 방법\n",
    "- Checkpoints를 사용하여 히스토리 관리 \n",
    "    - 메모리에 저장\n",
    "    - DB에 저장\n",
    "- 본 코드는 DB에 저장하여, 히스토리를 관리하는 방식임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e8d3b7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6a290cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "\n",
    "small_llm = ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "235ceb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_google_community import GmailToolkit\n",
    "from langchain_google_community.gmail.utils import (\n",
    "    build_resource_service,\n",
    "    get_gmail_credentials,\n",
    ")\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"숫자 a와 b를 더합니다.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"숫자 a와 b를 곱합니다.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "credentials = get_gmail_credentials(\n",
    "    token_file=\"./google/gmail_token.json\",\n",
    "    scopes=[\"https://mail.google.com/\"],\n",
    "    client_secrets_file=\"./google/gmail_credentials.json\",\n",
    ")\n",
    "api_resource = build_resource_service(credentials=credentials)\n",
    "gmail_toolkit = GmailToolkit(api_resource=api_resource)\n",
    "gmail_tool_list = gmail_toolkit.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6d42e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "loaded_tool_list = load_tools(\n",
    "    [\"arxiv\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b8eeef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_list = [add, multiply, search_tool] + gmail_tool_list + loaded_tool_list\n",
    "llm_with_tools = llm.bind_tools(tool_list)\n",
    "tool_node = ToolNode(tool_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6ab5b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "\n",
    "class AgentState(MessagesState): # 써머리도 같이 담아야함 → MessagesState를 상속받을 클래스 선언\n",
    "    summary: str\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7b5748db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state: MessagesState) -> MessagesState:\n",
    "\n",
    "    '''\n",
    "    에이전트 함수는 주어진 상태에서 메시지를 가져와\n",
    "    LLM과 도구를 사용하여 응답 메시지를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        state (MessagesState): 메시지 상태를 포함하는 state.   \n",
    "\n",
    "    Returns:\n",
    "        MessagesState: 응답 메시지를 포함하는 새로운 state.\n",
    "    '''\n",
    "\n",
    "    messages = state['messages']    #1  메세지 추출\n",
    "    response = llm_with_tools.invoke(messages)  #2 LLM & 도구 → 메세치 처리 및 응답 생성\n",
    "    return {'messages': [response]} #3 새로운 응답 메세지 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4da54b",
   "metadata": {},
   "source": [
    "기존 방식의 문제점 \n",
    "- llm_with_tools.invoke(messages) 과정에서, 전체 히스토리를 바탕으로 이루어짐\n",
    "- 굉장히 많은 토큰 수를 소비하게 되고, 이로 인해 비용이 증가하고 시간이 오래걸려 서비스 품질이 저하됨\n",
    "- 긴 대화에서 윗 단의 내용은 보통 최근 대화와 관련이 없을 가능성이 큼\n",
    "- 이부분에 대한 관리가 필요 → 삭제 (e.g. RemoveMessage, node) | 요약 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ae1ec844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_messages(state: AgentState):\n",
    "    \"\"\"\n",
    "    주어진 state의 메시지를 요약합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 메시지와 요약을 포함하는 state.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: 요약된 메시지를 포함하는 딕셔너리.\n",
    "    \"\"\"\n",
    "    # state에서 메시지와 요약을 가져옴\n",
    "    messages = state['messages']\n",
    "    summary = state['summary']\n",
    "    \n",
    "    # 요약 프롬프트 생성 \n",
    "    summary_prompt = f'summarize this chat history below: \\n\\nchat_history:{messages}'\n",
    "    \n",
    "    # 기존 요약이 있으면, 요약을 포함한 프롬프트 생성\n",
    "    if summary != '':\n",
    "        summary_prompt = f'''summarize this chat history below while looking at the summary of earlier conversations\n",
    "chat_history:{messages}\n",
    "summary:{summary}'''\n",
    "    \n",
    "    # LLM을 사용하여 요약\n",
    "    summary = small_llm.invoke(summary_prompt)\n",
    "    \n",
    "    # 요약 메시지 반환\n",
    "    return {'summary': summary.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a877ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "def delete_messages(state: AgentState):\n",
    "    messages = state['messages']\n",
    "    delete_messages = [RemoveMessage(id=message.id) for message in messages[:-3]]\n",
    "\n",
    "    return {'messages': delete_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4d0a2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: MessagesState):\n",
    "    messages = state['messages']    #1 메세지 추출\n",
    "    last_ai_message = messages[-1]  #2 AiMessage 가져와서 \n",
    "    if last_ai_message.tool_calls:     \n",
    "        return 'tools'  #3 도구 호출 포함 여부로부터, 'tools' || 'END' 반환\n",
    "    return 'end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9a8eb8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    last_ai_message = messages[-1]\n",
    "    \n",
    "    if last_ai_message.tool_calls:\n",
    "        return 'tools'\n",
    "    \n",
    "    return 'summarize_messages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "34a3d251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x176ecc140>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_node('agent', agent)\n",
    "graph_builder.add_node('tools', tool_node) \n",
    "graph_builder.add_node(delete_messages)\n",
    "graph_builder.add_node(summarize_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844864b1",
   "metadata": {},
   "source": [
    "리마인드 사항\n",
    "- 기존 방식에서 agent - _end_ 사이에 노드가 추가되어야함\n",
    "- 따라서, 엣지에서 tools condition 사용 불가\n",
    "- 아래 코드 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "151aa7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x176ecc140>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import START, END\n",
    "\n",
    "graph_builder.add_edge(START, 'agent')\n",
    "graph_builder.add_conditional_edges(\n",
    "    'agent',\n",
    "    should_continue,\n",
    "    ['tools', 'summarize_messages']\n",
    ")\n",
    "graph_builder.add_edge('tools', 'agent')\n",
    "graph_builder.add_edge('summarize_messages', 'delete_messages')\n",
    "graph_builder.add_edge('delete_messages', END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92fe07",
   "metadata": {},
   "source": [
    "체크포인트 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f203e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=checkpointer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "354630c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac07fe5b",
   "metadata": {},
   "source": [
    "쓰레드 관리 → config 지정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "389edd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "1839sh@gmail.com로 이메일을 보내기 전에 Attention Is All You Need라는 논문을 요약헤서 초안을 작성해주세요.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv (call_BEQGzYxUB3cA0RNOa74YarXr)\n",
      " Call ID: call_BEQGzYxUB3cA0RNOa74YarXr\n",
      "  Args:\n",
      "    query: Attention Is All You Need\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv\n",
      "\n",
      "Published: 2021-05-06\n",
      "Title: Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet\n",
      "Authors: Luke Melas-Kyriazi\n",
      "Summary: The strong performance of vision transformers on image classification and other vision tasks is often attributed to the design of their multi-head attention layers. However, the extent to which attention is responsible for this strong performance remains unclear. In this short report, we ask: is the attention layer even necessary? Specifically, we replace the attention layer in a vision transformer with a feed-forward layer applied over the patch dimension. The resulting architecture is simply a series of feed-forward layers applied over the patch and feature dimensions in an alternating fashion. In experiments on ImageNet, this architecture performs surprisingly well: a ViT/DeiT-base-sized model obtains 74.9\\% top-1 accuracy, compared to 77.9\\% and 79.9\\% for ViT and DeiT respectively. These results indicate that aspects of vision transformers other than attention, such as the patch embedding, may be more responsible for their strong performance than previously thought. We hope these results prompt the community to spend more time trying to understand why our current models are as effective as they are.\n",
      "\n",
      "Published: 2025-12-03\n",
      "Title: \"All You Need\" is Not All You Need for a Paper Title: On the Origins of a Scientific Meme\n",
      "Authors: Anton Alyakin\n",
      "Summary: The 2017 paper ''Attention Is All You Need'' introduced the Transformer architecture-and inadvertently spawned one of machine learning's most persistent naming conventions. We analyze 717 arXiv preprints containing ''All You Need'' in their titles (2009-2025), finding exponential growth ($R^2$ > 0.994) following the original paper, with 200 titles in 2025 alone. Among papers following the canonical ''X [Is] All You Need'' structure, ''Attention'' remains the most frequently claimed necessity (28 occurrences). Situating this phenomenon within memetic theory, we argue the pattern's success reflects competitive pressures in scientific communication that increasingly favor memorability over precision. Whether this trend represents harmless academic whimsy or symptomatic sensationalism, we leave-with appropriate self-awareness-to the reader.\n",
      "\n",
      "Published: 2018-06-28\n",
      "Title: Quit When You Can: Efficient Evaluation of Ensembles with Ordering Optimization\n",
      "Authors: Serena Wang, Maya Gupta, Seungil You\n",
      "Summary: Given a classifier ensemble and a set of examples to be classified, many examples may be confidently and accurately classified after only a subset of the base models in the ensemble are evaluated. This can reduce both mean latency and CPU while maintaining the high accuracy of the original ensemble. To achieve such gains, we propose jointly optimizing a fixed evaluation order of the base models and early-stopping thresholds. Our proposed objective is a combinatorial optimization problem, but we provide a greedy algorithm that achieves a 4-approximation of the optimal solution for certain cases. For those cases, this is also the best achievable polynomial time approximation bound unless $P = NP$. Experiments on benchmark and real-world problems show that the proposed Quit When You Can (QWYC) algorithm can speed-up average evaluation time by $2$x--$4$x, and is around $1.5$x faster than prior work. QWYC's joint optimization of ordering and thresholds also performed better in experiments than various fixed orderings, including gradient boosted trees' ordering.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "논문 \"Attention Is All You Need\"는 Transformer 아키텍처를 소개한 주요 연구로, 자연어 처리 분야에서 큰 혁신을 이끌어냈습니다. 주요 아이디어는 입력 시퀀스의 중요 부분에 집중하는 '어텐션 메커니즘'을 활용하여 복잡한 의존성을 효과적으로 캡처하는 것입니다. 이 논문은 RNN(순환 신경망)이나 LSTM과 같은 전통적인 모델들보다 더 효율적이고, 병렬 처리에 유리한 구조를 제안합니다. Transformer는 인코더와 디코더로 구성되며, 각 모듈은 멀티헤드 어텐션과 피드포워드 레이어로 이루어져 있습니다. 이 구조는 자연어 처리 외에도 이미지 처리, 음성 인식 등 다양한 분야에 응용되고 있습니다.\n",
      "\n",
      "이 요약을 바탕으로 이메일 초안을 작성하겠습니다.\n",
      "Tool Calls:\n",
      "  create_gmail_draft (call_Tf7m46TQCN1xHCN7vwVuJnT2)\n",
      " Call ID: call_Tf7m46TQCN1xHCN7vwVuJnT2\n",
      "  Args:\n",
      "    message: 논문 'Attention Is All You Need'는 Transformer 아키텍처를 소개하며 자연어 처리 분야에서 혁신을 가져왔습니다. 이 연구는 입력 시퀀스의 중요한 부분에 집중하는 '어텐션 메커니즘'을 통해 복잡한 의존성을 효과적으로 포착하는 방법을 제안합니다. RNN이나 LSTM과 같은 전통적인 모델들보다 효율적이며 병렬 처리에 유리한 구조로, 인코더와 디코더로 구성되어 있습니다. 멀티헤드 어텐션과 피드포워드 레이어로 이루어진 이 구조는 자연어 처리뿐만 아니라 이미지 처리, 음성 인식 등 다양한 분야에 활용되고 있습니다.\n",
      "    to: ['1839sh@gmail.com']\n",
      "    subject: Attention Is All You Need 논문 요약\n",
      "    cc: None\n",
      "    bcc: None\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: create_gmail_draft\n",
      "\n",
      "Draft created. Draft Id: r8299407415800062961\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "이메일 초안이 작성되었습니다. \"Attention Is All You Need\" 논문 요약을 포함한 이메일 초안이 준비되었습니다. 1839sh@gmail.com으로 이메일을 보내기 전에 초안을 검토할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\n",
    "    'configurable': {\n",
    "        'thread_id': 'paper_summary'\n",
    "    }\n",
    "}\n",
    "\n",
    "query = '1839sh@gmail.com로 이메일을 보내기 전에 Attention Is All You Need라는 논문을 요약헤서 초안을 작성해주세요.'\n",
    "for chunk in graph.stream({'messages': [HumanMessage(query)]}, config=config,stream_mode='values'):\n",
    "    chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b771050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='1839sh@gmail.com로 이메일을 보내기 전에 Attention Is All You Need라는 논문을 요약헤서 초안을 작성해주세요.', additional_kwargs={}, response_metadata={}, id='9ea3ed49-4ea0-40a9-b29c-0e80af51b6bc'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_BEQGzYxUB3cA0RNOa74YarXr', 'function': {'arguments': '{\"query\":\"Attention Is All You Need\"}', 'name': 'arxiv'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 724, 'total_tokens': 742, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_deacdd5f6f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-ff997584-bd22-4aa0-8bb3-4e024d2d9b36-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'Attention Is All You Need'}, 'id': 'call_BEQGzYxUB3cA0RNOa74YarXr', 'type': 'tool_call'}], usage_metadata={'input_tokens': 724, 'output_tokens': 18, 'total_tokens': 742, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " ToolMessage(content='Published: 2021-05-06\\nTitle: Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet\\nAuthors: Luke Melas-Kyriazi\\nSummary: The strong performance of vision transformers on image classification and other vision tasks is often attributed to the design of their multi-head attention layers. However, the extent to which attention is responsible for this strong performance remains unclear. In this short report, we ask: is the attention layer even necessary? Specifically, we replace the attention layer in a vision transformer with a feed-forward layer applied over the patch dimension. The resulting architecture is simply a series of feed-forward layers applied over the patch and feature dimensions in an alternating fashion. In experiments on ImageNet, this architecture performs surprisingly well: a ViT/DeiT-base-sized model obtains 74.9\\\\% top-1 accuracy, compared to 77.9\\\\% and 79.9\\\\% for ViT and DeiT respectively. These results indicate that aspects of vision transformers other than attention, such as the patch embedding, may be more responsible for their strong performance than previously thought. We hope these results prompt the community to spend more time trying to understand why our current models are as effective as they are.\\n\\nPublished: 2025-12-03\\nTitle: \"All You Need\" is Not All You Need for a Paper Title: On the Origins of a Scientific Meme\\nAuthors: Anton Alyakin\\nSummary: The 2017 paper \\'\\'Attention Is All You Need\\'\\' introduced the Transformer architecture-and inadvertently spawned one of machine learning\\'s most persistent naming conventions. We analyze 717 arXiv preprints containing \\'\\'All You Need\\'\\' in their titles (2009-2025), finding exponential growth ($R^2$ > 0.994) following the original paper, with 200 titles in 2025 alone. Among papers following the canonical \\'\\'X [Is] All You Need\\'\\' structure, \\'\\'Attention\\'\\' remains the most frequently claimed necessity (28 occurrences). Situating this phenomenon within memetic theory, we argue the pattern\\'s success reflects competitive pressures in scientific communication that increasingly favor memorability over precision. Whether this trend represents harmless academic whimsy or symptomatic sensationalism, we leave-with appropriate self-awareness-to the reader.\\n\\nPublished: 2018-06-28\\nTitle: Quit When You Can: Efficient Evaluation of Ensembles with Ordering Optimization\\nAuthors: Serena Wang, Maya Gupta, Seungil You\\nSummary: Given a classifier ensemble and a set of examples to be classified, many examples may be confidently and accurately classified after only a subset of the base models in the ensemble are evaluated. This can reduce both mean latency and CPU while maintaining the high accuracy of the original ensemble. To achieve such gains, we propose jointly optimizing a fixed evaluation order of the base models and early-stopping thresholds. Our proposed objective is a combinatorial optimization problem, but we provide a greedy algorithm that achieves a 4-approximation of the optimal solution for certain cases. For those cases, this is also the best achievable polynomial time approximation bound unless $P = NP$. Experiments on benchmark and real-world problems show that the proposed Quit When You Can (QWYC) algorithm can speed-up average evaluation time by $2$x--$4$x, and is around $1.5$x faster than prior work. QWYC\\'s joint optimization of ordering and thresholds also performed better in experiments than various fixed orderings, including gradient boosted trees\\' ordering.', name='arxiv', id='a2a7fdd4-4c15-4ca7-8234-a1a71edaa869', tool_call_id='call_BEQGzYxUB3cA0RNOa74YarXr'),\n",
       " AIMessage(content='논문 \"Attention Is All You Need\"는 Transformer 아키텍처를 소개한 주요 연구로, 자연어 처리 분야에서 큰 혁신을 이끌어냈습니다. 주요 아이디어는 입력 시퀀스의 중요 부분에 집중하는 \\'어텐션 메커니즘\\'을 활용하여 복잡한 의존성을 효과적으로 캡처하는 것입니다. 이 논문은 RNN(순환 신경망)이나 LSTM과 같은 전통적인 모델들보다 더 효율적이고, 병렬 처리에 유리한 구조를 제안합니다. Transformer는 인코더와 디코더로 구성되며, 각 모듈은 멀티헤드 어텐션과 피드포워드 레이어로 이루어져 있습니다. 이 구조는 자연어 처리 외에도 이미지 처리, 음성 인식 등 다양한 분야에 응용되고 있습니다.\\n\\n이 요약을 바탕으로 이메일 초안을 작성하겠습니다.', additional_kwargs={'tool_calls': [{'id': 'call_Tf7m46TQCN1xHCN7vwVuJnT2', 'function': {'arguments': '{\"message\":\"논문 \\'Attention Is All You Need\\'는 Transformer 아키텍처를 소개하며 자연어 처리 분야에서 혁신을 가져왔습니다. 이 연구는 입력 시퀀스의 중요한 부분에 집중하는 \\'어텐션 메커니즘\\'을 통해 복잡한 의존성을 효과적으로 포착하는 방법을 제안합니다. RNN이나 LSTM과 같은 전통적인 모델들보다 효율적이며 병렬 처리에 유리한 구조로, 인코더와 디코더로 구성되어 있습니다. 멀티헤드 어텐션과 피드포워드 레이어로 이루어진 이 구조는 자연어 처리뿐만 아니라 이미지 처리, 음성 인식 등 다양한 분야에 활용되고 있습니다.\",\"to\":[\"1839sh@gmail.com\"],\"subject\":\"Attention Is All You Need 논문 요약\",\"cc\":null,\"bcc\":null}', 'name': 'create_gmail_draft'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 399, 'prompt_tokens': 1461, 'total_tokens': 1860, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_deacdd5f6f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a00c7abe-700a-453b-8fa3-37289e80462e-0', tool_calls=[{'name': 'create_gmail_draft', 'args': {'message': \"논문 'Attention Is All You Need'는 Transformer 아키텍처를 소개하며 자연어 처리 분야에서 혁신을 가져왔습니다. 이 연구는 입력 시퀀스의 중요한 부분에 집중하는 '어텐션 메커니즘'을 통해 복잡한 의존성을 효과적으로 포착하는 방법을 제안합니다. RNN이나 LSTM과 같은 전통적인 모델들보다 효율적이며 병렬 처리에 유리한 구조로, 인코더와 디코더로 구성되어 있습니다. 멀티헤드 어텐션과 피드포워드 레이어로 이루어진 이 구조는 자연어 처리뿐만 아니라 이미지 처리, 음성 인식 등 다양한 분야에 활용되고 있습니다.\", 'to': ['1839sh@gmail.com'], 'subject': 'Attention Is All You Need 논문 요약', 'cc': None, 'bcc': None}, 'id': 'call_Tf7m46TQCN1xHCN7vwVuJnT2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1461, 'output_tokens': 399, 'total_tokens': 1860, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " ToolMessage(content='Draft created. Draft Id: r8299407415800062961', name='create_gmail_draft', id='528f4609-11f7-4482-8ce0-439da05c12de', tool_call_id='call_Tf7m46TQCN1xHCN7vwVuJnT2'),\n",
       " AIMessage(content='이메일 초안이 작성되었습니다. \"Attention Is All You Need\" 논문 요약을 포함한 이메일 초안이 준비되었습니다. 1839sh@gmail.com으로 이메일을 보내기 전에 초안을 검토할 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 1888, 'total_tokens': 1938, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1792}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_deacdd5f6f', 'finish_reason': 'stop', 'logprobs': None}, id='run-6c460e79-c819-43d1-b9ea-6e13de106757-0', usage_metadata={'input_tokens': 1888, 'output_tokens': 50, 'total_tokens': 1938, 'input_token_details': {'audio': 0, 'cache_read': 1792}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_message_list = graph.get_state(config).values['messages']\n",
    "current_message_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b090a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "for index, message in enumerate(current_message_list):\n",
    "    if index < len(current_message_list) -1:\n",
    "        graph.update_state(config, {'messages': RemoveMessage(id=message.id)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cfa58010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='이메일 초안이 작성되었습니다. \"Attention Is All You Need\" 논문 요약을 포함한 이메일 초안이 준비되었습니다. 1839sh@gmail.com으로 이메일을 보내기 전에 초안을 검토할 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 1888, 'total_tokens': 1938, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1792}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_deacdd5f6f', 'finish_reason': 'stop', 'logprobs': None}, id='run-6c460e79-c819-43d1-b9ea-6e13de106757-0', usage_metadata={'input_tokens': 1888, 'output_tokens': 50, 'total_tokens': 1938, 'input_token_details': {'audio': 0, 'cache_read': 1792}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_message_list = graph.get_state(config).values['messages']\n",
    "current_message_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18430407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\n",
    "    'configurable': {\n",
    "        'thread_id': 'paper_summary'\n",
    "    }\n",
    "}\n",
    "\n",
    "query = '1839sh@gmail.com로 이메일을 보내기 전에 Attention Is All You Need라는 논문을 요약헤서 초안을 작성해주세요.'\n",
    "for chunk in graph.stream({'messages': [HumanMessage(query)]}, config=config,stream_mode='values'):\n",
    "    chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e93060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "해당 논문의 출처 URL을 이메일로 보내주세요\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv (call_iMb4m3dSnlwudgPI2Rsg2j7I)\n",
      " Call ID: call_iMb4m3dSnlwudgPI2Rsg2j7I\n",
      "  Args:\n",
      "    query: Attention Is All You Need\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv\n",
      "\n",
      "Published: 2021-05-06\n",
      "Title: Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet\n",
      "Authors: Luke Melas-Kyriazi\n",
      "Summary: The strong performance of vision transformers on image classification and other vision tasks is often attributed to the design of their multi-head attention layers. However, the extent to which attention is responsible for this strong performance remains unclear. In this short report, we ask: is the attention layer even necessary? Specifically, we replace the attention layer in a vision transformer with a feed-forward layer applied over the patch dimension. The resulting architecture is simply a series of feed-forward layers applied over the patch and feature dimensions in an alternating fashion. In experiments on ImageNet, this architecture performs surprisingly well: a ViT/DeiT-base-sized model obtains 74.9\\% top-1 accuracy, compared to 77.9\\% and 79.9\\% for ViT and DeiT respectively. These results indicate that aspects of vision transformers other than attention, such as the patch embedding, may be more responsible for their strong performance than previously thought. We hope these results prompt the community to spend more time trying to understand why our current models are as effective as they are.\n",
      "\n",
      "Published: 2025-12-03\n",
      "Title: \"All You Need\" is Not All You Need for a Paper Title: On the Origins of a Scientific Meme\n",
      "Authors: Anton Alyakin\n",
      "Summary: The 2017 paper ''Attention Is All You Need'' introduced the Transformer architecture-and inadvertently spawned one of machine learning's most persistent naming conventions. We analyze 717 arXiv preprints containing ''All You Need'' in their titles (2009-2025), finding exponential growth ($R^2$ > 0.994) following the original paper, with 200 titles in 2025 alone. Among papers following the canonical ''X [Is] All You Need'' structure, ''Attention'' remains the most frequently claimed necessity (28 occurrences). Situating this phenomenon within memetic theory, we argue the pattern's success reflects competitive pressures in scientific communication that increasingly favor memorability over precision. Whether this trend represents harmless academic whimsy or symptomatic sensationalism, we leave-with appropriate self-awareness-to the reader.\n",
      "\n",
      "Published: 2018-06-28\n",
      "Title: Quit When You Can: Efficient Evaluation of Ensembles with Ordering Optimization\n",
      "Authors: Serena Wang, Maya Gupta, Seungil You\n",
      "Summary: Given a classifier ensemble and a set of examples to be classified, many examples may be confidently and accurately classified after only a subset of the base models in the ensemble are evaluated. This can reduce both mean latency and CPU while maintaining the high accuracy of the original ensemble. To achieve such gains, we propose jointly optimizing a fixed evaluation order of the base models and early-stopping thresholds. Our proposed objective is a combinatorial optimization problem, but we provide a greedy algorithm that achieves a 4-approximation of the optimal solution for certain cases. For those cases, this is also the best achievable polynomial time approximation bound unless $P = NP$. Experiments on benchmark and real-world problems show that the proposed Quit When You Can (QWYC) algorithm can speed-up average evaluation time by $2$x--$4$x, and is around $1.5$x faster than prior work. QWYC's joint optimization of ordering and thresholds also performed better in experiments than various fixed orderings, including gradient boosted trees' ordering.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\"Attention Is All You Need\" 논문에 대한 특정 URL 정보를 찾을 수 없었습니다. 논문 제목으로 검색한 결과 여러 논문이 나왔으며, 각 논문은 다양한 주제를 다루고 있습니다. 구체적인 정보를 위해 직접 arXiv 웹사이트에서 검색하시거나, 추가적인 세부사항을 제공해 주시면 더 많은 도움을 드릴 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "update_query = '이메일로 보내주세요'\n",
    "for chunk in graph.stream({'messages': [HumanMessage(update_query)]}, config=config,stream_mode='values'):\n",
    "    chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8485713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inflearn_langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
